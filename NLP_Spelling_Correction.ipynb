{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* Write a parser that can read all the lines of the file holbrook.txt and print out for each line the original (misspelled) text, the corrected text and the indexes of any changes. The indexes refers to the index of the words in the sentence. In the example given, there is only an error in the 10th word and so the list of indexes is [9]. It is not necessary to analyze where the error occurs inside the word.\n",
        "\n",
        "Then split your data into a test set of 100 lines and a training set."
      ],
      "metadata": {
        "id": "hV4-LEwWAbB9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAmpEFGL_8xP"
      },
      "outputs": [],
      "source": [
        "# Importing all required libraries for this task.\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.metrics.distance import edit_distance\n",
        "from nltk.corpus import words\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from itertools import chain\n",
        "import json\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "from nltk.stem import *\n",
        "from nltk.corpus import wordnet as wn\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from difflib import SequenceMatcher\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dE4PIAktBDne",
        "outputId": "a0e07b73-0acf-47e6-8151-f3f91e7cc71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parsing(sent):  \n",
        "    \"\"\"Parsing the sentence to corrected and original and storing in the dictionary.\"\"\"\n",
        "    loriginal = []\n",
        "    lcorrected = []\n",
        "    lcorr = []\n",
        "    indexes = []\n",
        "    cnt = 0\n",
        "    \n",
        "    for i in sent:\n",
        "        if '|' in i:\n",
        "            # Splitting the sentence on '|'\n",
        "            str1 = i.split('|')\n",
        "            # Previous word to '|' is storing in loriginal list.\n",
        "            loriginal.append(str1[0])\n",
        "            # Next word to '|' is storing in lcorrected list.\n",
        "            lcorrected.append(str1[1])\n",
        "            #Noting down the index of error.\n",
        "            indexes.append(cnt)\n",
        "        \n",
        "        else:\n",
        "            # If there is no '|' in sentence, sentence is stored in loriginal and lcorrected as it is.\n",
        "            loriginal.append(i)\n",
        "            lcorrected.append(i)\n",
        "        cnt = cnt+1\n",
        "        \n",
        "    #Loading to loriginal, lcorrected and index list to dictionary.      \n",
        "    dictionary = {'original': loriginal, 'corrected': lcorrected, 'indexes': indexes}\n",
        "    \n",
        "    return dictionary\n",
        "\n",
        "def preprocessing():\n",
        "    \"\"\"Loading the data from 'holbrook.txt' and passing to parsing function to get parssed sentences. \n",
        "    Returning the whole dictionary as data.\"\"\"\n",
        "    data = []\n",
        "    \n",
        "    # Reading the txt file\n",
        "    text_file = open(\"holbrook.txt\", \"r\")\n",
        "    lines = []\n",
        "    for i in text_file:\n",
        "        lines.append(i.strip())\n",
        "    \n",
        "    # Word tokenizing the sentences\n",
        "    sentences = [nltk.word_tokenize(sent) for sent in lines]\n",
        "    \n",
        "    # Calling a parse function to get corrected, original sentences.\n",
        "    for sent in sentences:\n",
        "        data.append(parsing(sent))\n",
        "    \n",
        "    return data\n",
        "\n",
        "#Calling preprocessing function\n",
        "data = preprocessing()\n",
        "\n",
        "# Testing\n",
        "print(data[2])\n",
        "assert(data[2] == {\n",
        " 'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'],\n",
        " 'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'],\n",
        " 'indexes': [9]\n",
        "})\n",
        "\n",
        "# Splitting the data to test 100 lines and remaining training lines\n",
        "test = data[:100]\n",
        "train = data[100:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0MYKX03AmB4",
        "outputId": "516a6748-5d07-40e9-f7ba-dac08fc54a46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'], 'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'], 'indexes': [9]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The counts and assertions given in the following sections are based on splitting the training and test set as follows**"
      ],
      "metadata": {
        "id": "P2iwKKqLBOa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data to test - first 100 lines and remaining training lines\n",
        "def test_train_split():\n",
        "    \"\"\"Splitting the data to test - first 100 lines and remaining training lines.\"\"\"\n",
        "    test = data[:100]\n",
        "    train = data[100:]\n",
        "    \n",
        "    # Seperating the train original, test original, test corrected and train corrected from dictionary to list.\n",
        "    train_corrected = [elem['corrected'] for elem in train]\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    test_corrected = [elem['corrected'] for elem in test]\n",
        "    test_original = [elem['original'] for elem in test]\n",
        "    \n",
        "    # Removing all special characters from the list.\n",
        "    test_original = [tokenizer.tokenize(\" \".join(elem)) for elem in test_original]\n",
        "    test_corrected = [tokenizer.tokenize(\" \".join(elem)) for elem in test_corrected]\n",
        "    train_corrected = [tokenizer.tokenize(\" \".join(elem)) for elem in train_corrected]\n",
        "    \n",
        "    return test_corrected, test_original, train_corrected\n",
        "\n",
        "# Test and Training data.\n",
        "test_corrected, test_original, train_corrected = test_train_split()"
      ],
      "metadata": {
        "id": "tbhe7gRtAoBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate the frequency (number of occurrences), ignoring case, of all words and bigrams (sequences of two words) from the corrected training sentences:**"
      ],
      "metadata": {
        "id": "EdmgVSHeBROG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unigram(words):\n",
        "    \"\"\"This function returns a unigram frequency for a given word.\"\"\"\n",
        "    doc = []\n",
        "    words = words.lower()\n",
        "    for i in train_corrected:\n",
        "        doc.append(\" \".join(i).lower())\n",
        "\n",
        "    doc = \" \".join(doc)    \n",
        "    doc = nltk.word_tokenize(doc)\n",
        "    \n",
        "    # Calculates frequency distribution.\n",
        "    unig_freq = nltk.FreqDist(doc)\n",
        "    \n",
        "    # This gives word count - which is not used (for future modification)\n",
        "    tnum_unig = sum(unig_freq.values())\n",
        "    \n",
        "    return unig_freq[words], tnum_unig\n",
        "\n",
        "e, f = unigram('me')\n",
        "print(\"unigram('me')==\", e)\n",
        "    \n",
        "def bigram(words):\n",
        "    \"\"\"This function returns a bigram frequency for a given words.\"\"\"\n",
        "    doc = []\n",
        "    \n",
        "    # This function get words in string, hence converting string of 2 words to tuple.\n",
        "    words = words.split(\" \")\n",
        "    words[0] = words[0].lower()\n",
        "    words[1] = words[1].lower()\n",
        "    words = tuple(words)\n",
        "    \n",
        "    for i in train_corrected:\n",
        "        doc.append(\" \".join(i))\n",
        "        \n",
        "    doc = \" \".join(doc)    \n",
        "    doc = doc.lower()\n",
        "    \n",
        "    #Calculating Bigrams for given words.\n",
        "    tokens = nltk.wordpunct_tokenize(doc)\n",
        "    bigrams=nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
        "    biag_freq = dict(bigrams.ngram_fd)\n",
        "    \n",
        "    # This gives totla bigram count - which is not used (for future modification)\n",
        "    tnum_bg = sum(biag_freq.values())\n",
        "    \n",
        "    # If there is no such bigram return 0\n",
        "    try:\n",
        "        return biag_freq[words], tnum_bg\n",
        "    except KeyError:\n",
        "        return 0, 0\n",
        "    \n",
        "    \n",
        "a, b = bigram(\"my mother\")\n",
        "print(\"bigram('my mother')==\", a)\n",
        "c, d = bigram(\"you did\")\n",
        "print(\"bigram('you did')==\", c)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0aXKli5BJUd",
        "outputId": "779dff31-1b47-4f94-de9d-9ec5d051c4b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unigram('me')== 87\n",
            "bigram('my mother')== 17\n",
            "bigram('you did')== 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Edit distance is a method that calculates how similar two strings are to one another by counting the minimum number of operations required to transform one string into the other. There is a built-in implementation in NLTK that works as follows:**"
      ],
      "metadata": {
        "id": "wMhuWdzBBi4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "# Edit distance returns the number of changes to transform one word to another\n",
        "print(edit_distance(\"hello\", \"hi\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU0E93GzBbsr",
        "outputId": "60423050-bc6e-4933-be30-4c30746cf8e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write a function that calculates all words with minimal edit distance to the misspelled word.**\n",
        "\n",
        "You should do this as follows\n",
        "\n",
        "1.Collect the set of all unique tokens in train\n",
        "2.Find the minimal edit distance, that is the lowest value for the function edit_distance between token and a word in train\n",
        "3.Output all unique words in train that have this same (minimal) edit_distance value\n",
        "\n",
        "**Do not implement edit distance, use the built-in NLTK function edit_distance**"
      ],
      "metadata": {
        "id": "ogX6X7CsBn4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_candidates(token):\n",
        "    \n",
        "    \"\"\"Get nearest word for a given incorrect word.\"\"\"\n",
        "    doc = []\n",
        "\n",
        "    for i in train_corrected:\n",
        "        doc.append(\" \".join(i))\n",
        "\n",
        "    doc = \" \".join(doc)\n",
        "    doc = nltk.word_tokenize(doc)\n",
        "    unig_freq = nltk.FreqDist(doc)\n",
        "    unique_words = list(unig_freq.keys())\n",
        "\n",
        "    # Calculate distance between two words\n",
        "    s = []\n",
        "    for i in unique_words:\n",
        "        t = edit_distance(i, token)\n",
        "        s.append(t)\n",
        "    \n",
        "    # Store the neares words in ordered dictionary\n",
        "    dist = dict(zip(unique_words, s))\n",
        "    dist_sorted = dict(sorted(dist.items(), key=lambda x:x[1]))\n",
        "    minimal_dist = list(dist_sorted.values())[0]\n",
        "    \n",
        "    keys_min = list(filter(lambda k: dist_sorted[k] == minimal_dist, dist_sorted.keys()))\n",
        "    \n",
        "    return keys_min\n",
        "\n",
        "print(get_candidates(\"minde\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LbjrI3sBls7",
        "outputId": "3121ed12-19ec-4a21-a981-6844ea6b53e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mine', 'mind']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a function that takes a (misspelled) sentence and returns the corrected version of that sentence. The system should scan the sentence for words that are not in the dictionary (set of unique words in the training set) and for each word that is not in the dictionary choose a word in the dictionary that has minimal edit distance and has the highest bigram probability. That is the candidate should be selected using the previous and following word in a bigram language model. Thus, if the th word in a sentence is misspelled we should use the following to rank candidates:\n",
        "\n",
        "       p(wi+1}w;)p(w;|wi1)\n",
        "      \n",
        "\n",
        "For the first and last word of the sentence use only the conditional probabilities that exist.\n",
        "\n",
        "Your solution to this should involve get_candidates"
      ],
      "metadata": {
        "id": "HlSjn2tpB5Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is to culculate unigram and bigram probabilities in correct function\n",
        "doc = []\n",
        "\n",
        "for i in train_corrected:\n",
        "    doc.append(\" \".join(i).lower())\n",
        "\n",
        "doc = \" \".join(doc)\n",
        "doc = nltk.word_tokenize(doc)\n",
        "unig_freq = nltk.FreqDist(doc)\n",
        "unique_words = list(unig_freq.keys())\n",
        "\n",
        "cf_biag = nltk.ConditionalFreqDist(nltk.bigrams(doc))\n",
        "cf_biag = nltk.ConditionalProbDist(cf_biag, nltk.MLEProbDist)\n"
      ],
      "metadata": {
        "id": "TthlUKKwB2Vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correct(sentence):\n",
        "    \"This function returns the corrected sentence based on bigram probability.\"\n",
        "    corrected = []\n",
        "    cnt = 0\n",
        "    indexes = []\n",
        "   \n",
        "    for i in sentence:\n",
        "        # If word not in unique word the calculate suggested words with minimal distance\n",
        "        if i.lower() not in unique_words:\n",
        "            indexes.append(cnt)\n",
        "            if len(get_candidates(i)) > 1:\n",
        "\n",
        "                suggestion = get_candidates(i)\n",
        "                prob = []\n",
        "            \n",
        "            # For each suggested word calaculate bigram probability\n",
        "                for sug in suggestion:\n",
        "\n",
        "                    # Check the misspelled word is first or last word of the sentence\n",
        "                    if ((cnt != 0) and (cnt != len(sentence)-1)):\n",
        "                    \n",
        "                        p1 = cf_biag[sug.lower()].prob(sentence[cnt+1].lower())\n",
        "                        p2 = cf_biag[corrected[len(corrected)-1].lower()].prob(sug.lower())\n",
        "                        p = p1 * p2\n",
        "                        prob.append(p)  \n",
        "                    else:\n",
        "                        #If mispelled word is last word of a sencence take probaility of previous word\n",
        "                        if cnt == len(sentence)-1:\n",
        "                            \n",
        "                            p2 = cf_biag[corrected[len(corrected)-1].lower()].prob(sug.lower())\n",
        "                            prob.append(p2)\n",
        "                        #If mispelled word is first word of a sencence take probaility of next word\n",
        "                        elif cnt == 0:\n",
        "                        \n",
        "                            p1 = cf_biag[sug.lower()].prob(sentence[cnt+1].lower())\n",
        "                            prob.append(p1)\n",
        "\n",
        "                 \n",
        "                # Take the suggested word with maximum priobability.\n",
        "                if len(suggestion[prob.index(max(prob))]) > 1:\n",
        "                    corrected.append(suggestion[prob.index(max(prob))])\n",
        "                else:\n",
        "                    corrected.append(suggestion[prob.index(max(prob))])\n",
        "            # If only 1 suggested word take that word - no need to calculate probabilities\n",
        "            else:\n",
        "                corrected.append(get_candidates(i)[0])\n",
        "\n",
        "        else:\n",
        "            corrected.append(i)\n",
        "    #Return the corrected sentence\n",
        "        cnt = cnt+1\n",
        "    return corrected\n",
        "\n",
        "\n",
        "print(correct([\"this\",\"whitr\",\"cat\"]))\n",
        "assert(correct([\"this\",\"whitr\",\"cat\"]) == ['this','white','cat'])\n",
        "\n",
        "                    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr-iqsvhC5aU",
        "outputId": "3e87fae1-ea2d-426f-c202-ad1962dab38b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', 'white', 'cat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using the test corpus evaluate the accuracy of your method, i.e., how many words from your system's output match the corrected sentence (you should count words that are already spelled correctly and not changed by the system).**"
      ],
      "metadata": {
        "id": "B7gVkhHeDNHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "def accuracy(actual_sent, sent_pred):\n",
        "    \"\"\"This is based on word to word accuracy calculation. Compares each word with the actual word and calculate accuracy\"\"\"\n",
        "    actual = actual_sent\n",
        "    predict = correct(sent_pred)\n",
        "    # If the blank sentence i.e for a blank line predicted is also blank take accuracy as 1\n",
        "    if len(actual) == 0 and len(predict)==0:\n",
        "        accuracy = 1.0\n",
        "    else:\n",
        "        # Take all predicted words same as actual word and divide by lenght of sentence\n",
        "        accuracy = len(set(predict) & set(actual))/len(set(actual))\n",
        "    \n",
        "    #print(\"Actual Sentence: \", actual)\n",
        "    #print(\"Sentence to predict: \", sent_pred)\n",
        "    #print(\"Predicted Sentence: \", predict)\n",
        "    #print(\"Accuracy: \", accuracy)\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "acc = []\n",
        "for i in tqdm(range(len(test_corrected))):\n",
        "    acc.append(accuracy(test_corrected[i], test_original[i]))\n",
        "    \n",
        "    \n",
        "print(\"************************************************EVALUATION**********************************************************\")\n",
        "print(\"Average Accuracy of words in each sentence: \", round(sum(acc)/len(acc)*100, 4), \"%\")\n",
        "print(acc.count(1), \"out of 100 sentences predicted correctly without any error.\")\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"Elapsed Time is: \", elapsed_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wW9f_W8DJl_",
        "outputId": "226205c9-1d1b-4c2d-aad5-21a32ffc3c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:11<00:00,  1.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************************************EVALUATION**********************************************************\n",
            "Average Accuracy of words in each sentence:  82.7862 %\n",
            "24 out of 100 sentences predicted correctly without any error.\n",
            "Elapsed Time is:  71.87659311294556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Smoothing :**\n",
        "\n",
        "Can also implement different smoothing techniques to avoid zero brobability."
      ],
      "metadata": {
        "id": "LlSe4Tm6YQhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tense(suggestion, sentence):    \n",
        "    \"\"\"Tense Detection\"\"\"\n",
        "    tag = dict(nltk.pos_tag(sentence)).values()\n",
        "    past_tense = ['VBN', 'VBD']\n",
        "    conti_tense = ['VBG']\n",
        "    \n",
        "    # If sentence is past tense append ed and check if it is valid word\n",
        "    if any(x in tag for x in past_tense):\n",
        "        sug = []\n",
        "        for a in suggestion:\n",
        "            if a.lower()+'ed' in unique_words:\n",
        "                sug.append(a+'ed')\n",
        "        for aelem in sug:\n",
        "            suggestion.append(aelem)\n",
        "            \n",
        "    # If sentence is past tense append ed and check if it is valid word\n",
        "    if any(x in tag for x in conti_tense):\n",
        "        sug = []\n",
        "        for b in suggestion:\n",
        "            if b.lower()+'ing' in unique_words:\n",
        "                sug.append(b+'ing')\n",
        "        for belem in sug:\n",
        "            suggestion.append(belem)\n",
        "        \n",
        "    return suggestion\n",
        "\n",
        "def named_entity(sentence):\n",
        "    \"\"\"Named Entity Detection using nltk.pos_tag and nltk.ne_chunk\"\"\"\n",
        "    l = []\n",
        "    for chunk in nltk.ne_chunk(nltk.pos_tag(sentence)):\n",
        "        # If any named tag like PERSON, ORGANIZATION or GEOLOCATION append to list.\n",
        "          if hasattr(chunk, 'label'):\n",
        "            l.append(' '.join(c[0] for c in chunk))\n",
        "    \n",
        "    if len(l) != 0:\n",
        "        l = \" \".join(l)\n",
        "        l = l.split(\" \")\n",
        "        \n",
        "    return l\n",
        "\n",
        "#print(named_entity(['I', 'live', 'at', 'Boar', 'Parva', 'it', 'is', 'near', 'Melton', 'and', 'Bridgebrook', 'and', 'Smallerden']))\n",
        "\n",
        "\n",
        "def word_forms_new(suggest):\n",
        "    \"\"\"Taking different forms of words using derivationally related forms\"\"\"\n",
        "    sug_form = []\n",
        "    for w in suggest:\n",
        "        forms = set()\n",
        "        for i in wn.lemmas(w):\n",
        "            forms.add(i.name())\n",
        "            for j in i.derivationally_related_forms():\n",
        "                forms.add(j.name())\n",
        "        \n",
        "        for a in list(forms):\n",
        "            sug_form.append(a)\n",
        "    \n",
        "    for q in sug_form:\n",
        "        suggest.append(q)\n",
        "    \n",
        "    word_forms = []\n",
        "    [word_forms.append(i) for i in suggest if not i in word_forms]\n",
        "    return word_forms\n",
        "\n",
        "def conditions(corrected, cr_ind):\n",
        "    \"\"\"Common word - Oclock is not detecting. Hence handling manually but not necessary\"\"\"\n",
        "    \n",
        "    if 'oclock' in corrected:\n",
        "        ind = corrected.index('oclock')\n",
        "        corrected = list(map(lambda x: str.replace(x, \"oclock\", \"clock\"), corrected))\n",
        "        corrected.insert(ind, 'o')\n",
        "        return corrected\n",
        "        \n",
        "    return corrected\n",
        "#word_forms_new(['wait', 'said', 'laid', 'paid', 'wad', 'waited'])\n",
        "\n",
        "def sentence_sentence_similarity(sentence1):\n",
        "    \"\"\"Sentence - Sentence similarity using sequence matcher. We can also use cosine similarity but not implemented\"\"\"\n",
        "    correc = []\n",
        "    for d in train_corrected:\n",
        "        ratio = SequenceMatcher(None, \" \".join(d), \" \".join(sentence1)).ratio()\n",
        "        if ratio > 98:\n",
        "            correc.append(d)\n",
        "    \n",
        "    if len(correc) == 1:\n",
        "        return correc[0]\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "#sentence_sentence_similarity(['1'])"
      ],
      "metadata": {
        "id": "4cZ7qCCaYGGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgxWJVuYYzy7",
        "outputId": "2fd0e831-0b0b-4587-b331-617521f72f9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt27VpQaY-kd",
        "outputId": "ebd05b69-2852-4a55-b3c8-ad517898e6a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs2HXqEMZFAX",
        "outputId": "d343e286-dd80-4d06-8308-203758a5a4b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_mod(sentence):\n",
        "    sts = ['oclock']\n",
        "    corrected = []\n",
        "    cnt = 0\n",
        "    indexes = []\n",
        "    #To check stemmed word in dictonary or not\n",
        "    stemmer = PorterStemmer()\n",
        "    status = 0\n",
        "    #This will extract all named entities of a sentence\n",
        "    n_en = named_entity(sentence)\n",
        "    \n",
        "    for i in sentence:\n",
        "        # Check for sentence similarity\n",
        "        corr = sentence_sentence_similarity(i)\n",
        "        if len(corr) == 1:\n",
        "            return corr\n",
        "        # Ignoring digits like page number and lemmatizing the word and check if it is present in dictionary and use words.words() for word validation.\n",
        "        elif i.lower() not in unique_words and not i.isdigit() and lemmatizer.lemmatize(i.lower()) not in unique_words and i not in n_en and i not in sts and i not in wn.words() and stemmer.stem(i) not in wn.words():\n",
        "            indexes.append(cnt)\n",
        "            if len(get_candidates(i)) > 1:\n",
        "                # Get words forms, tense detection for suggested sentence\n",
        "                suggestion = get_candidates(i)\n",
        "                suggestion = tense(suggestion, sentence)\n",
        "                wd_fms = word_forms_new(suggestion)\n",
        "                suggestion = wd_fms\n",
        "\n",
        "                prob = []\n",
        "                \n",
        "                # Bigram probabilities\n",
        "                for sug in suggestion:\n",
        "\n",
        "                    # Check the misspelled word is first or last word of the sentence\n",
        "                    if ((cnt != 0) and (cnt != len(sentence)-1)):\n",
        "\n",
        "                        try:\n",
        "                            p1 = cf_biag[sug.lower()].prob(sentence[cnt+1].lower())\n",
        "                            p2 = cf_biag[corrected[len(corrected)-1].lower()].prob(sug.lower())\n",
        "                            p = p1 * p2\n",
        "                            prob.append(p)   \n",
        "                        except:\n",
        "                            prob.append(0)\n",
        "                    else:\n",
        "                        #If mispelled word is last word of a sencence take probaility of previous word\n",
        "                        if cnt == len(sentence)-1:\n",
        "                            try:\n",
        "                                p2 = cf_biag[corrected[len(corrected)-1].lower()].prob(sug.lower())\n",
        "                                prob.append(p2)\n",
        "                            except:\n",
        "                                prob.append(0)\n",
        "\n",
        "\n",
        "                        elif cnt == 0:\n",
        "                            #If mispelled word is first word of a sencence take probaility of next word\n",
        "                            try:\n",
        "                                p1 = cf_biag[sug.lower()].prob(sentence[cnt+1].lower())\n",
        "                                prob.append(p1)\n",
        "                            except:\n",
        "                                prob.append(0)\n",
        "              \n",
        "                if len(suggestion[prob.index(max(prob))]) > 1:\n",
        "                    corrected.append(suggestion[prob.index(max(prob))])\n",
        "                else:\n",
        "                    corrected.append(suggestion[prob.index(max(prob))])\n",
        "\n",
        "            else:\n",
        "                corrected.append(get_candidates(i)[0])\n",
        "\n",
        "        else:\n",
        "            corrected.append(i)\n",
        "\n",
        "        cnt = cnt+1\n",
        "        # Manula hadling 'Oclock'\n",
        "        corrected = conditions(corrected, indexes)\n",
        "    \n",
        "    fin = sentence_sentence_similarity(corrected)\n",
        "    if len(fin) != 0:\n",
        "        return fin\n",
        "    else:\n",
        "        return corrected\n",
        "\n",
        "\n",
        "print(correct_mod(['My', 'Mum', 'goe', 'out', 'some_times']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPq_Y92KYgLv",
        "outputId": "20bf2347-a934-4438-ec22-2db2dfd603a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['My', 'Mum', 'got', 'out', 'sometimes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Evaluation:**\n",
        "\n",
        "From all above mentioned implementation ideas, it is observed that the accuracy has been increased to approximately 92%. Which is 10% increase from Task 3 and Task 4. Moreover, I have also cross checked for different test and train split and repeated the process and noted down the accuracy. It is observed the accuracy is between [89% to 93%].\n",
        "\n",
        "**Note:** Actual and Predicted sentence can be displayed by uncommenting the below print statement.\n",
        "\n",
        "**Results**\n",
        "\n",
        "* Average Accuracy of words in each sentence: 91.0187 % .\n",
        "\n",
        "* 42 out of 100 sentences predicted correctly without any error.\n",
        "\n",
        "* Elapsed Time is: 212.06087279319763."
      ],
      "metadata": {
        "id": "zKiezh32Zbi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "def accuracy(actual_sent, sent_pred):\n",
        "    \"\"\"This is based on word to word accuracy calculation. Compares each word with the actual word and calculate accuracy\"\"\"\n",
        "    actual = actual_sent\n",
        "    predict = correct_mod(sent_pred)\n",
        "    # If the blank sentence i.e for a blank line predicted is also blank take accuracy as 1\n",
        "    if len(actual) == 0 and len(predict)==0:\n",
        "        accuracy = 1.0\n",
        "    else:\n",
        "        # Take all predicted words same as actual word and divide by lenght of sentence\n",
        "        accuracy = len(set(predict) & set(actual))/len(set(actual))\n",
        "    \n",
        "    #print(\"Actual Sentence: \", actual)\n",
        "    #print(\"Sentence to predict: \", sent_pred)\n",
        "    #print(\"Predicted Sentence: \", predict)\n",
        "    #print(\"Accuracy: \", accuracy)\n",
        "    \n",
        "    return accuracy\n",
        "    \n",
        "\n",
        "acc = []\n",
        "for i in tqdm(range(len(test_corrected))):\n",
        "    acc.append(accuracy(test_corrected[i], test_original[i]))\n",
        "\n",
        "print(\"************************************************EVALUATION**********************************************************\")\n",
        "print(\"Average Accuracy of words in each sentence: \", round(sum(acc)/len(acc)*100, 4), \"%\")\n",
        "print(acc.count(1), \"out of 100 sentences predicted correctly without any error\")\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"Elapsed Time is: \", elapsed_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SK_Dl4ajYtUv",
        "outputId": "88df3fc4-8977-4d7c-b4a5-642a2a7d74f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [03:00<00:00,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************************************EVALUATION**********************************************************\n",
            "Average Accuracy of words in each sentence:  91.0187 %\n",
            "42 out of 100 sentences predicted correctly without any error\n",
            "Elapsed Time is:  180.66126990318298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yxBtImc2Zmva"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}